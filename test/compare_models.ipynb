{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "521b1a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "006471bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ana/ACS/rag/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if \"../src\" not in sys.path:\n",
    "    sys.path.append(\"../src\")\n",
    "import settings\n",
    "import json\n",
    "import hashlib\n",
    "from typing import List\n",
    "import utils\n",
    "import os\n",
    "\n",
    "from retriever import Retriever\n",
    "from reranker import Reranker\n",
    "from chatbot import Chatbot\n",
    "from langchain_openai import ChatOpenAI\n",
    "from rag_evaluator import RAGEvaluator\n",
    "import time\n",
    "\n",
    "logger = utils.get_logger(\"evaluate\")\n",
    "\n",
    "k = 5\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0229f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(filename: str, obj: dict) -> None:\n",
    "    with open(filename, \"w\") as file:\n",
    "        json.dump(obj, file, indent=4, ensure_ascii=False, sort_keys=True)\n",
    "\n",
    "def load_json(filename) -> List[dict]:\n",
    "    try:\n",
    "        with open(filename, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "def compute_entry_id(entry) -> str:\n",
    "    key_data = {\n",
    "        \"query\": entry[\"user_input\"],\n",
    "        \"answer\": entry[\"response\"],\n",
    "        \"context\": sorted(entry[\"retrieved_ids\"]),\n",
    "    }\n",
    "    key_string = json.dumps(key_data, separators=(\",\", \":\"), sort_keys=True)\n",
    "    \n",
    "    return hashlib.sha256(key_string.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def get_chatbot_responses(chatbot: Chatbot, questions: List[str], filename: str, rewrite_query: bool = False) -> None:\n",
    "    queries = questions\n",
    "\n",
    "    # load available data\n",
    "    responses = load_json(filename) or []\n",
    "    available_questions = [entry[\"user_input\"] for entry in responses]\n",
    "    queries = [question for question in questions if question not in available_questions]\n",
    "\n",
    "    for question in queries:\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        chatbot.run(query=question, rewrite_query=rewrite_query)\n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        response = {\n",
    "            \"user_input\": question,\n",
    "            \"response\": chatbot.get_response(),\n",
    "            \"retrieved_contexts\": chatbot.get_context(),\n",
    "            \"retrieved_ids\": chatbot.get_documents_ids(),\n",
    "            \"retrieved_scores\": chatbot.get_retrieved_scores(),\n",
    "            \"response_time\": elapsed_time\n",
    "        }\n",
    "        id = compute_entry_id(response)\n",
    "        response[\"id\"] = id\n",
    "        responses.append(response)\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "    save_json(filename, responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff2a3f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = load_json(f\"data/questions.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abb6552",
   "metadata": {},
   "source": [
    "#### Run chatbot with gpt-4o-mini, save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0653a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever()\n",
    "reranker = Reranker()\n",
    "chatbot_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=settings.TEMPERATURE, top_p=settings.TOP_P)\n",
    "chatbot = Chatbot(retriever=retriever, reranker=reranker, llm=chatbot_llm, k=k)\n",
    "\n",
    "# Run chatbot with reranker\n",
    "get_chatbot_responses(chatbot=chatbot, questions=questions, filename=f\"logs/gpt-4o-mini/qa.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa99fc1a",
   "metadata": {},
   "source": [
    "#### Run chatbot with deepseek, save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "186fb472",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGS_DIR = f\"logs/deepseek\"\n",
    "\n",
    "retriever = Retriever()\n",
    "reranker = Reranker()\n",
    "chatbot_llm = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=settings.TEMPERATURE,\n",
    "    top_p=settings.TOP_P,\n",
    "    openai_api_key=\"sk-c59800a5739b4026a62d1aca27657267\",\n",
    "    openai_api_base=\"https://api.deepseek.com\"\n",
    ")\n",
    "chatbot = Chatbot(retriever=retriever, reranker=reranker, llm=chatbot_llm, k=k)\n",
    "\n",
    "get_chatbot_responses(chatbot=chatbot, questions=questions, filename=f\"logs/deepseek/qa.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e75137",
   "metadata": {},
   "source": [
    "#### Run chatbot with Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "56ac384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "class GemmaLLM:\n",
    "    def __init__(self, model: str, temperature: float):\n",
    "        self.client = genai.Client(api_key=\"AIzaSyB3FMFxSCWrKqaxjpA0K2F1hOn1aOIBIOI\")\n",
    "        self.model_name = model\n",
    "        self.temperature=temperature\n",
    "\n",
    "    class LLMResponse:\n",
    "        def __init__(self, content: str):\n",
    "            self.content = content\n",
    "\n",
    "    def invoke(self, prompt: str) -> 'GemmaLLM.LLMResponse':\n",
    "        response = self.client.models.generate_content(\n",
    "            model=self.model_name,\n",
    "            contents=prompt,\n",
    "            config=genai.types.GenerateContentConfig(\n",
    "                temperature=self.temperature\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return self.LLMResponse(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "793d4c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gemma-3n-e4b-it\"\n",
    "\n",
    "retriever = Retriever()\n",
    "reranker = Reranker()\n",
    "chatbot_llm = GemmaLLM(model_name, temperature=settings.TEMPERATURE)\n",
    "chatbot = Chatbot(retriever=retriever, reranker=reranker, llm=chatbot_llm, k=k)\n",
    "\n",
    "# Run chatbot with reranker\n",
    "get_chatbot_responses(chatbot=chatbot, questions=questions, filename=f\"logs/{model_name}/qa.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a6b3db",
   "metadata": {},
   "source": [
    "### Evaluate responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd706321",
   "metadata": {},
   "source": [
    "## gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294556be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-4o-mini\"\n",
    "evaluator = RAGEvaluator(k=k)\n",
    "evaluator.logs_dir = os.path.join(settings.BASE_DIR, \"test\", f\"logs/{model_name}\")\n",
    "evaluator.results_dir = os.path.join(settings.BASE_DIR, \"test\", f\"results/{model_name}\")\n",
    "\n",
    "data = load_json(f\"logs/{model_name}/qa.json\")\n",
    "evaluator.load_data(data)\n",
    "\n",
    "results[model_name] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ff1d3",
   "metadata": {},
   "source": [
    "- GPT Score - Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f786bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gpt-4o-mini] faithfulness GPT Score: 0.8400\n"
     ]
    }
   ],
   "source": [
    "faithfulness_gpt_score = evaluator.compute_faithfulness_gpt_score()\n",
    "results[model_name][\"faithfulness_gpt_score\"] = faithfulness_gpt_score\n",
    "print(f\"[{model_name}] faithfulness GPT Score: {faithfulness_gpt_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b670ff5",
   "metadata": {},
   "source": [
    "- GPT Score - Answer relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154a2605",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_relevance_gpt_score = evaluator.compute_answer_relevance_gpt_score()\n",
    "results[model_name][\"answer_relevance_gpt_score\"] = answer_relevance_gpt_score\n",
    "print(f\"[{model_name}] answer relevance GPT Score: {answer_relevance_gpt_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0beda9",
   "metadata": {},
   "source": [
    "- RAGAs - Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e20f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_ragas = await evaluator.compute_faithfulness_ragas()\n",
    "results[model_name][\"faithfulness_ragas\"] = faithfulness_ragas\n",
    "print(f\"[{model_name}] Faithfulness RAGAs: {faithfulness_ragas:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f15b2",
   "metadata": {},
   "source": [
    "- RAGAs - Answer Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155367bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_relevance_ragas = await evaluator.compute_answer_relevance_ragas()\n",
    "results[model_name][\"answer_relevance_ragas\"] = answer_relevance_ragas\n",
    "print(f\"[{model_name}] Answer relevance RAGAs: {answer_relevance_ragas:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda83a21",
   "metadata": {},
   "source": [
    "## deepsek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45bb01d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RAGEvaluator(k=k)\n",
    "evaluator.logs_dir = os.path.join(settings.BASE_DIR, \"test\", f\"logs/deepseek\")\n",
    "evaluator.results_dir = os.path.join(settings.BASE_DIR, \"test\", f\"results/deepseek\")\n",
    "model_name = \"deepseek\"\n",
    "\n",
    "data = load_json(f\"logs/deepseek/qa.json\")\n",
    "evaluator.load_data(data)\n",
    "\n",
    "results[model_name] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c5b450",
   "metadata": {},
   "source": [
    "- GPT Score - Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9961e824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deepseek] faithfulness GPT Score: 0.9375\n"
     ]
    }
   ],
   "source": [
    "faithfulness_gpt_score = evaluator.compute_faithfulness_gpt_score()\n",
    "results[model_name][\"faithfulness_gpt_score\"] = faithfulness_gpt_score\n",
    "print(f\"[{model_name}] faithfulness GPT Score: {faithfulness_gpt_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be1ccaf",
   "metadata": {},
   "source": [
    "- GPT Score - Answer Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39ca5a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[deepseek] answer relevance GPT Score: 0.7650\n"
     ]
    }
   ],
   "source": [
    "answer_relevance_gpt_score = evaluator.compute_answer_relevance_gpt_score()\n",
    "results[model_name][\"answer_relevance_gpt_score\"] = answer_relevance_gpt_score\n",
    "print(f\"[{model_name}] answer relevance GPT Score: {answer_relevance_gpt_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48b5740",
   "metadata": {},
   "source": [
    "- RAGAs - Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76e8049d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saving prompts: The file '/home/ana/ACS/rag/test/prompts/faithfulness_n_l_i_statement_prompt_romanian.json' already exists.\n",
      "All entries already evaluated.\n",
      "[deepseek] Faithfulness RAGAs: 0.7523\n"
     ]
    }
   ],
   "source": [
    "faithfulness_ragas = await evaluator.compute_faithfulness_ragas()\n",
    "results[model_name][\"faithfulness_ragas\"] = faithfulness_ragas\n",
    "print(f\"[{model_name}] Faithfulness RAGAs: {faithfulness_ragas:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a52165",
   "metadata": {},
   "source": [
    "- RAGAs - Answer Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "819b3c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saving prompts: The file '/home/ana/ACS/rag/test/prompts/answer_relevancy_response_relevance_prompt_romanian.json' already exists.\n",
      "All entries already evaluated.\n",
      "[deepseek] Answer relevance RAGAs: 0.5138\n"
     ]
    }
   ],
   "source": [
    "answer_relevance_ragas = await evaluator.compute_answer_relevance_ragas()\n",
    "results[model_name][\"answer_relevance_ragas\"] = answer_relevance_ragas\n",
    "print(f\"[{model_name}] Answer relevance RAGAs: {answer_relevance_ragas:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7a8b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(f\"results/{model_name}/results.json\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b141d3",
   "metadata": {},
   "source": [
    "## gemma-3n-e4b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78273654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ana/ACS/rag/test/rag_evaluator.py:28: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embedding_model = LangchainEmbeddingsWrapper(HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"))\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gemma-3n-e4b-it\"\n",
    "evaluator = RAGEvaluator(k=k)\n",
    "evaluator.logs_dir = os.path.join(settings.BASE_DIR, \"test\", f\"logs/{model_name}\")\n",
    "evaluator.results_dir = os.path.join(settings.BASE_DIR, \"test\", f\"results/{model_name}\")\n",
    "\n",
    "data = load_json(f\"logs/{model_name}/qa.json\")\n",
    "evaluator.load_data(data)\n",
    "\n",
    "results[model_name] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893f00a3",
   "metadata": {},
   "source": [
    "- GPT Score - Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2794c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gemma-3n-e4b-it] faithfulness GPT Score: 0.8400\n"
     ]
    }
   ],
   "source": [
    "faithfulness_gpt_score = evaluator.compute_faithfulness_gpt_score()\n",
    "results[model_name][\"faithfulness_gpt_score\"] = faithfulness_gpt_score\n",
    "print(f\"[{model_name}] faithfulness GPT Score: {faithfulness_gpt_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433afee0",
   "metadata": {},
   "source": [
    "- GPT Score - Answer Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52430b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gemma-3n-e4b-it] answer relevance GPT Score: 0.6300\n"
     ]
    }
   ],
   "source": [
    "answer_relevance_gpt_score = evaluator.compute_answer_relevance_gpt_score()\n",
    "results[model_name][\"answer_relevance_gpt_score\"] = answer_relevance_gpt_score\n",
    "print(f\"[{model_name}] answer relevance GPT Score: {answer_relevance_gpt_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa5f989",
   "metadata": {},
   "source": [
    "- RAGAs - Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcf6ce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saving prompts: The file '/home/ana/ACS/rag/test/prompts/faithfulness_n_l_i_statement_prompt_romanian.json' already exists.\n",
      "All entries already evaluated.\n",
      "[gemma-3n-e4b-it] Faithfulness RAGAs: 0.6643\n"
     ]
    }
   ],
   "source": [
    "faithfulness_ragas = await evaluator.compute_faithfulness_ragas()\n",
    "results[model_name][\"faithfulness_ragas\"] = faithfulness_ragas\n",
    "print(f\"[{model_name}] Faithfulness RAGAs: {faithfulness_ragas:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44315b8",
   "metadata": {},
   "source": [
    "- RAGAs - Answer Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d9d0e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saving prompts: The file '/home/ana/ACS/rag/test/prompts/answer_relevancy_response_relevance_prompt_romanian.json' already exists.\n",
      "All entries already evaluated.\n",
      "[gemma-3n-e4b-it] Answer relevance RAGAs: 0.5124\n"
     ]
    }
   ],
   "source": [
    "answer_relevance_ragas = await evaluator.compute_answer_relevance_ragas()\n",
    "results[model_name][\"answer_relevance_ragas\"] = answer_relevance_ragas\n",
    "print(f\"[{model_name}] Answer relevance RAGAs: {answer_relevance_ragas:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1706360",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(f\"results/{model_name}/results.json\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a30df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
