{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "dc1fc744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "058a5584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"../src\" not in sys.path:\n",
    "    sys.path.append(\"../src\")\n",
    "import settings\n",
    "import json\n",
    "import hashlib\n",
    "from typing import List\n",
    "import utils\n",
    "import os\n",
    "\n",
    "from retriever import Retriever\n",
    "from reranker import Reranker\n",
    "from chatbot import Chatbot\n",
    "from langchain_openai import ChatOpenAI\n",
    "from rag_evaluator import RAGEvaluator\n",
    "\n",
    "logger = utils.get_logger(\"evaluate\")\n",
    "\n",
    "k = 5\n",
    "LOGS_DIR = f\"logs/k={k}\"\n",
    "RESULTS_DIR = f\"results/k={k}\"\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "06a3cd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(filename: str, obj: dict) -> None:\n",
    "    with open(filename, \"w\") as file:\n",
    "        json.dump(obj, file, indent=4, ensure_ascii=False, sort_keys=True)\n",
    "\n",
    "def load_json(filename) -> List[dict]:\n",
    "    try:\n",
    "        with open(filename, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "def save_result(name: str, value: float):\n",
    "    with open(f\"{RESULTS_DIR}/{name}.json\", \"w\") as f:\n",
    "        json.dump({name: value}, f, indent=4)\n",
    "\n",
    "def compute_entry_id(entry) -> str:\n",
    "    key_data = {\n",
    "        \"query\": entry[\"user_input\"],\n",
    "        \"answer\": entry[\"response\"],\n",
    "        \"context\": sorted(entry[\"retrieved_ids\"]),\n",
    "    }\n",
    "    key_string = json.dumps(key_data, separators=(\",\", \":\"), sort_keys=True)\n",
    "    \n",
    "    return hashlib.sha256(key_string.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def get_chatbot_responses(chatbot: Chatbot, questions: List[str], filename: str, rewrite_query: bool = False) -> None:\n",
    "    queries = questions\n",
    "\n",
    "    # load available data\n",
    "    responses = load_json(filename) or []\n",
    "    available_questions = [entry[\"user_input\"] for entry in responses]\n",
    "    queries = [question for question in questions if question not in available_questions]\n",
    "\n",
    "    for question in queries:\n",
    "        chatbot.run(query=question, rewrite_query=rewrite_query)\n",
    "        response = {\n",
    "            \"user_input\": question,\n",
    "            \"response\": chatbot.get_response(),\n",
    "            \"retrieved_contexts\": chatbot.get_context(),\n",
    "            \"retrieved_ids\": chatbot.get_documents_ids(),\n",
    "            \"retrieved_scores\": chatbot.get_retrieved_scores(),\n",
    "        }\n",
    "        id = compute_entry_id(response)\n",
    "        response[\"id\"] = id\n",
    "        responses.append(response)\n",
    "\n",
    "    save_json(filename, responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a26027e",
   "metadata": {},
   "source": [
    "Load questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a2ed1d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load questions\n",
    "questions = load_json(f\"data/questions.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c800c913",
   "metadata": {},
   "source": [
    "#### Run chatbot with retriever only, save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "797e9291",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever(k=k)\n",
    "chatbot_llm = ChatOpenAI(model=settings.OPENAI_MODEL, temperature=settings.TEMPERATURE)\n",
    "chatbot = Chatbot(retriever=retriever, reranker=None, llm=chatbot_llm, k=k)\n",
    "\n",
    "# Run chatbot with retriever only\n",
    "chatbot.rerank = False\n",
    "get_chatbot_responses(chatbot=chatbot, questions=questions, filename=f\"{LOGS_DIR}/qa_retriever.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0779b70f",
   "metadata": {},
   "source": [
    "#### Run chatbot with reranker, save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "fe8ace24",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever(k=15)\n",
    "reranker = Reranker()\n",
    "chatbot_llm = ChatOpenAI(model=settings.OPENAI_MODEL, temperature=settings.TEMPERATURE)\n",
    "chatbot = Chatbot(retriever=retriever, reranker=reranker, llm=chatbot_llm, k=k)\n",
    "\n",
    "# Run chatbot with reranker\n",
    "get_chatbot_responses(chatbot=chatbot, questions=questions, filename=f\"{LOGS_DIR}/qa_reranker.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389ee70a",
   "metadata": {},
   "source": [
    "### 1. Retriever evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "0d8dae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_evaluator = RAGEvaluator(k=k)\n",
    "data = load_json(f\"{LOGS_DIR}/qa_retriever.json\")\n",
    "retriever_evaluator.load_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07937e51",
   "metadata": {},
   "source": [
    "1.1 Retriever MAP@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "aec76e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever MAP@7: 0.7151\n"
     ]
    }
   ],
   "source": [
    "retriever_map_at_k = retriever_evaluator.compute_map_at_k()\n",
    "results[\"retriever_map_at_k\"] = retriever_map_at_k\n",
    "print(f\"Retriever MAP@{k}: {retriever_map_at_k:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e8f1a4",
   "metadata": {},
   "source": [
    "1.2 Retriever MRR@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "65e74856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever MRR@7: 0.7327\n"
     ]
    }
   ],
   "source": [
    "retriever_mrr_at_k = retriever_evaluator.compute_mrr_at_k()\n",
    "results[\"retriever_mrr_at_k\"] = retriever_mrr_at_k\n",
    "print(f\"Retriever MRR@{k}: {retriever_mrr_at_k:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97189f6c",
   "metadata": {},
   "source": [
    "### 2. Reranker evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "8896211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RAGEvaluator(k=k)\n",
    "data = load_json(f\"{LOGS_DIR}/qa_reranker.json\")\n",
    "evaluator.load_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f7168b",
   "metadata": {},
   "source": [
    "2.1 Reranker MAP@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "061782c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranker MAP@7: 0.8155\n"
     ]
    }
   ],
   "source": [
    "reranker_map_at_k = evaluator.compute_map_at_k()\n",
    "results[\"reranker_map_at_k\"] = reranker_map_at_k\n",
    "print(f\"Reranker MAP@{k}: {reranker_map_at_k:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eda9db1",
   "metadata": {},
   "source": [
    "2.2 Reranker MRR@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "613efa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranker MRR@7: 0.8479\n"
     ]
    }
   ],
   "source": [
    "reranker_mrr_at_k = evaluator.compute_mrr_at_k()\n",
    "results[\"reranker_mrr_at_k\"] = reranker_mrr_at_k\n",
    "print(f\"Reranker MRR@{k}: {reranker_mrr_at_k:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3144a7fc",
   "metadata": {},
   "source": [
    "### 3. End-to-end system evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594d8647",
   "metadata": {},
   "source": [
    "3.1 GPT Score - Faithfulness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55331b0d",
   "metadata": {},
   "source": [
    "Retriever-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "af10bc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever faithfulness GPT Score: 0.5975\n"
     ]
    }
   ],
   "source": [
    "retriever_faithfulness_gpt_score = retriever_evaluator.compute_faithfulness_gpt_score()\n",
    "results[\"retriever_faithfulness_gpt_score\"] = retriever_faithfulness_gpt_score\n",
    "print(f\"Retriever faithfulness GPT Score: {retriever_faithfulness_gpt_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c98182",
   "metadata": {},
   "source": [
    "Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "deb15852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness GPT Score: 0.7275\n"
     ]
    }
   ],
   "source": [
    "faithfulness_gpt_score = evaluator.compute_faithfulness_gpt_score()\n",
    "results[\"faithfulness_gpt_score\"] = faithfulness_gpt_score\n",
    "print(f\"Faithfulness GPT Score: {faithfulness_gpt_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd0234b",
   "metadata": {},
   "source": [
    "#### 3.2 GPT Score - Answer relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126029e3",
   "metadata": {},
   "source": [
    "Retriever-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "464a9997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever answer relevance GPT Score: 0.6025\n"
     ]
    }
   ],
   "source": [
    "retriever_answer_relevance_gpt_score = retriever_evaluator.compute_answer_relevance_gpt_score()\n",
    "results[\"retriever_answer_relevance_gpt_score\"] = retriever_answer_relevance_gpt_score\n",
    "print(f\"Retriever answer relevance GPT Score: {retriever_answer_relevance_gpt_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aa037a",
   "metadata": {},
   "source": [
    "Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "ff0b9290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer relevance GPT Score: 0.7050\n"
     ]
    }
   ],
   "source": [
    "answer_relevance_gpt_score = evaluator.compute_answer_relevance_gpt_score()\n",
    "results[\"answer_relevance_gpt_score\"] = answer_relevance_gpt_score\n",
    "print(f\"Answer relevance GPT Score: {answer_relevance_gpt_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263af8a6",
   "metadata": {},
   "source": [
    "#### 3.3 RAGAs - Faithfulness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c22691",
   "metadata": {},
   "source": [
    "Retriever only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "cf537d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saving prompts: The file '/home/ana/ACS/rag/test/prompts/faithfulness_n_l_i_statement_prompt_romanian.json' already exists.\n",
      "All entries already evaluated.\n",
      "Retriever faithfulness RAGAs: 0.7812\n"
     ]
    }
   ],
   "source": [
    "retriever_faithfulness_ragas = await retriever_evaluator.compute_faithfulness_ragas()\n",
    "results[\"retriever_faithfulness_ragas\"] = retriever_faithfulness_ragas\n",
    "print(f\"Retriever faithfulness RAGAs: {retriever_faithfulness_ragas:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d44aaa",
   "metadata": {},
   "source": [
    "Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "b93aed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saving prompts: The file '/home/ana/ACS/rag/test/prompts/faithfulness_n_l_i_statement_prompt_romanian.json' already exists.\n",
      "All entries already evaluated.\n",
      "Faithfulness RAGAs: 0.8128\n"
     ]
    }
   ],
   "source": [
    "faithfulness_ragas = await evaluator.compute_faithfulness_ragas()\n",
    "results[\"faithfulness_ragas\"] = faithfulness_ragas\n",
    "print(f\"Faithfulness RAGAs: {faithfulness_ragas:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d6a05c",
   "metadata": {},
   "source": [
    "#### 3.4 RAGAs - Answer relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4481aa",
   "metadata": {},
   "source": [
    "Retriever only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "a5efb1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saving prompts: The file '/home/ana/ACS/rag/test/prompts/answer_relevancy_response_relevance_prompt_romanian.json' already exists.\n",
      "All entries already evaluated.\n",
      "Retriever answer relevance RAGAs: 0.4724\n"
     ]
    }
   ],
   "source": [
    "retriever_answer_relevance_ragas = await retriever_evaluator.compute_answer_relevance_ragas()\n",
    "results[\"retriever_answer_relevance_ragas\"] = retriever_answer_relevance_ragas\n",
    "print(f\"Retriever answer relevance RAGAs: {retriever_answer_relevance_ragas:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da2b857",
   "metadata": {},
   "source": [
    "Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "04d227f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saving prompts: The file '/home/ana/ACS/rag/test/prompts/answer_relevancy_response_relevance_prompt_romanian.json' already exists.\n",
      "All entries already evaluated.\n",
      "Answer relevance RAGAs: 0.5720\n"
     ]
    }
   ],
   "source": [
    "answer_relevance_ragas = await evaluator.compute_answer_relevance_ragas()\n",
    "results[\"answer_relevance_ragas\"] = answer_relevance_ragas\n",
    "print(f\"Answer relevance RAGAs: {answer_relevance_ragas:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "798d795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(f\"{RESULTS_DIR}/results.json\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8e416b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9293cda0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb61ca",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "43d6f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check_questions = load_json(\"data/sanity_check_questions.json\")\n",
    "sanity_check_answers = load_json(\"data/sanity_check_answers.json\")\n",
    "question_mapping = {question: sanity_check_questions[i] for i, question in enumerate(questions)}\n",
    "answers_mapping = {question: sanity_check_answers[i] for i, question in enumerate(questions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "2fd0aba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize sanity check evaluator, uses different logs and results directories\n",
    "sanity_check_evaluator = RAGEvaluator(k=k)\n",
    "sanity_check_evaluator.logs_dir = os.path.join(settings.BASE_DIR, \"test\", \"logs\", \"sanity_check\")\n",
    "sanity_check_evaluator.results_dir = os.path.join(settings.BASE_DIR, \"test\", \"results\", \"sanity_check\")\n",
    "\n",
    "# load data: replace user queries with random, out of domain, sanity check questions\n",
    "sanity_check_data = load_json(f\"{LOGS_DIR}/qa_retriever.json\")\n",
    "for item in sanity_check_data:\n",
    "    item[\"user_input\"] = question_mapping[item[\"user_input\"]]\n",
    "sanity_check_evaluator.load_data(sanity_check_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c799ec69",
   "metadata": {},
   "source": [
    "Retriever MAP@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "ed050258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check retriever MAP@5: 0.0000\n"
     ]
    }
   ],
   "source": [
    "sanity_check_retriever_map_at_k = sanity_check_evaluator.compute_map_at_k()\n",
    "results[\"sanity_check_retriever_map_at_k\"] = sanity_check_retriever_map_at_k\n",
    "print(f\"Sanity check retriever MAP@{k}: {sanity_check_retriever_map_at_k:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0385e03a",
   "metadata": {},
   "source": [
    "Retriever MRR@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dce381d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check Retriever MRR@5: 0.0000\n"
     ]
    }
   ],
   "source": [
    "sanity_check_retriever_mrr_at_k = sanity_check_evaluator.compute_mrr_at_k()\n",
    "results[\"sanity_check_retriever_mrr_at_k\"] = sanity_check_retriever_mrr_at_k\n",
    "print(f\"Sanity check Retriever MRR@{k}: {sanity_check_retriever_mrr_at_k:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e281c",
   "metadata": {},
   "source": [
    "#### Reranker evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00099504",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "dfbf0adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize sanity check evaluator, uses different logs and results directories\n",
    "sanity_check_evaluator = RAGEvaluator(k=k)\n",
    "sanity_check_evaluator.logs_dir = os.path.join(settings.BASE_DIR, \"test\", \"logs\", \"sanity_check\")\n",
    "sanity_check_evaluator.results_dir = os.path.join(settings.BASE_DIR, \"test\", \"results\", \"sanity_check\")\n",
    "\n",
    "# load data: replace user queries with random, out of domain, sanity check questions\n",
    "sanity_check_data = load_json(f\"{LOGS_DIR}/qa_reranker.json\")\n",
    "for item in sanity_check_data:\n",
    "    item[\"user_input\"] = question_mapping[item[\"user_input\"]]\n",
    "sanity_check_evaluator.load_data(sanity_check_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d82738",
   "metadata": {},
   "source": [
    "Reranker MAP@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "0f2fddd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check Reranker MAP@5: 0.0000\n"
     ]
    }
   ],
   "source": [
    "sanity_check_reranker_map_at_k = sanity_check_evaluator.compute_map_at_k()\n",
    "results[\"sanity_check_reranker_map_at_k\"] = sanity_check_reranker_map_at_k\n",
    "print(f\"Sanity check Reranker MAP@{k}: {sanity_check_reranker_map_at_k:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bd2e1d",
   "metadata": {},
   "source": [
    "Reranker MRR@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e0b4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check Reranker MRR@5: 0.0000\n"
     ]
    }
   ],
   "source": [
    "sanity_check_reranker_mrr_at_k = sanity_check_evaluator.compute_mrr_at_k()\n",
    "results[\"sanity_check_reranker_mrr_at_k\"] = sanity_check_reranker_mrr_at_k\n",
    "print(f\"Sanity check Reranker MRR@{k}: {sanity_check_reranker_mrr_at_k:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dbe967",
   "metadata": {},
   "source": [
    "End-to-end system evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce36ed9c",
   "metadata": {},
   "source": [
    "Retriever only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "ce9e0fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize sanity check evaluator, uses different logs and results directories\n",
    "retriever_sanity_check_evaluator = RAGEvaluator(k=k)\n",
    "retriever_sanity_check_evaluator.logs_dir = os.path.join(settings.BASE_DIR, \"test\", \"logs\", \"sanity_check\")\n",
    "retriever_sanity_check_evaluator.results_dir = os.path.join(settings.BASE_DIR, \"test\", \"results\", \"sanity_check\")\n",
    "\n",
    "# load data: replace chatbot answers with random, out of domain, sanity check answers\n",
    "sanity_check_data = load_json(f\"{LOGS_DIR}/qa_retriever.json\")\n",
    "for item in sanity_check_data:\n",
    "    item[\"response\"] = answers_mapping[item[\"user_input\"]]\n",
    "retriever_sanity_check_evaluator.load_data(sanity_check_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcac6c3e",
   "metadata": {},
   "source": [
    "Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "862ff8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize sanity check evaluator, uses different logs and results directories\n",
    "sanity_check_evaluator = RAGEvaluator(k=k)\n",
    "sanity_check_evaluator.logs_dir = os.path.join(settings.BASE_DIR, \"test\", \"logs\", \"sanity_check\")\n",
    "sanity_check_evaluator.results_dir = os.path.join(settings.BASE_DIR, \"test\", \"results\", \"sanity_check\")\n",
    "\n",
    "# load data: replace chatbot answers with random, out of domain, sanity check answers\n",
    "sanity_check_data = load_json(f\"{LOGS_DIR}/qa_retriever.json\")\n",
    "for item in sanity_check_data:\n",
    "    item[\"response\"] = answers_mapping[item[\"user_input\"]]\n",
    "sanity_check_evaluator.load_data(sanity_check_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ebe070",
   "metadata": {},
   "source": [
    "#### End-to-end system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b4743d",
   "metadata": {},
   "source": [
    "Faithfulness - GPT Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31076b85",
   "metadata": {},
   "source": [
    "Retriever only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "d652351a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever only sanity check Faithfulness GPT Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "retriever_sanity_check_faithfulness_gpt_score = retriever_sanity_check_evaluator.compute_faithfulness_gpt_score()\n",
    "results[\"retriever_sanity_check_faithfulness_gpt_score\"] = retriever_sanity_check_faithfulness_gpt_score\n",
    "print(f\"Retriever only sanity check Faithfulness GPT Score: {retriever_sanity_check_faithfulness_gpt_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cce8cd",
   "metadata": {},
   "source": [
    "Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "4a3bdbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Faithfulness GPT Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "sanity_check_faithfulness_gpt_score = sanity_check_evaluator.compute_faithfulness_gpt_score()\n",
    "results[\"sanity_check_faithfulness_gpt_score\"] = sanity_check_faithfulness_gpt_score\n",
    "print(f\"Sanity Check Faithfulness GPT Score: {sanity_check_faithfulness_gpt_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55068aed",
   "metadata": {},
   "source": [
    "Answer relevance - GPT Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1335f87d",
   "metadata": {},
   "source": [
    "Retriever only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "d3800d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever only Sanity check Answer relevance GPT Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "retriever_sanity_check_answer_relevance_gpt_score = retriever_sanity_check_evaluator.compute_answer_relevance_gpt_score()\n",
    "results[\"retriever_sanity_check_answer_relevance_gpt_score\"] = retriever_sanity_check_answer_relevance_gpt_score\n",
    "print(f\"Retriever only Sanity check Answer relevance GPT Score: {retriever_sanity_check_answer_relevance_gpt_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453d72fe",
   "metadata": {},
   "source": [
    "Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "e3fd0693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check Answer relevance GPT Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "sanity_check_answer_relevance_gpt_score = sanity_check_evaluator.compute_answer_relevance_gpt_score()\n",
    "results[\"sanity_check_answer_relevance_gpt_score\"] = sanity_check_answer_relevance_gpt_score\n",
    "print(f\"Sanity check Answer relevance GPT Score: {sanity_check_answer_relevance_gpt_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8910c87a",
   "metadata": {},
   "source": [
    "Faithfulness - RAGAs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a07b43a",
   "metadata": {},
   "source": [
    "Retriever only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "b4a67dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saving prompts: The file '/home/ana/ACS/rag/test/prompts/faithfulness_n_l_i_statement_prompt_romanian.json' already exists.\n",
      "All entries already evaluated.\n",
      "Retriever only Sanity check Faithfulness RAGAs: 0.0000\n"
     ]
    }
   ],
   "source": [
    "retriever_sanity_check_faithfulness_ragas = await retriever_sanity_check_evaluator.compute_faithfulness_ragas()\n",
    "results[\"retriever_sanity_check_faithfulness_ragas\"] = retriever_sanity_check_faithfulness_ragas\n",
    "print(f\"Retriever only Sanity check Faithfulness RAGAs: {retriever_sanity_check_faithfulness_ragas:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d146754",
   "metadata": {},
   "source": [
    "Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "7cc6fc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saving prompts: The file '/home/ana/ACS/rag/test/prompts/faithfulness_n_l_i_statement_prompt_romanian.json' already exists.\n",
      "All entries already evaluated.\n",
      "Sanity check Faithfulness RAGAs: 0.0000\n"
     ]
    }
   ],
   "source": [
    "sanity_check_faithfulness_ragas = await sanity_check_evaluator.compute_faithfulness_ragas()\n",
    "results[\"sanity_check_faithfulness_ragas\"] = sanity_check_faithfulness_ragas\n",
    "print(f\"Sanity check Faithfulness RAGAs: {sanity_check_faithfulness_ragas:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6083a662",
   "metadata": {},
   "source": [
    "Answer relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25a7de7",
   "metadata": {},
   "source": [
    "Retriever only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "c021771f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saving prompts: The file '/home/ana/ACS/rag/test/prompts/answer_relevancy_response_relevance_prompt_romanian.json' already exists.\n",
      "All entries already evaluated.\n",
      "Retriever only sanity check Answer relevance RAGAs: 0.1027\n"
     ]
    }
   ],
   "source": [
    "retriever_sanity_check_answer_relevance_ragas = await retriever_sanity_check_evaluator.compute_answer_relevance_ragas()\n",
    "results[\"retriever_sanity_check_answer_relevance_ragas\"] = retriever_sanity_check_answer_relevance_ragas\n",
    "print(f\"Retriever only sanity check Answer relevance RAGAs: {retriever_sanity_check_answer_relevance_ragas:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f00af",
   "metadata": {},
   "source": [
    "Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "b93e31a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error saving prompts: The file '/home/ana/ACS/rag/test/prompts/answer_relevancy_response_relevance_prompt_romanian.json' already exists.\n",
      "All entries already evaluated.\n",
      "Sanity check Answer relevance RAGAs: 0.1027\n"
     ]
    }
   ],
   "source": [
    "sanity_check_answer_relevance_ragas = await sanity_check_evaluator.compute_answer_relevance_ragas()\n",
    "results[\"sanity_check_answer_relevance_ragas\"] = sanity_check_answer_relevance_ragas\n",
    "print(f\"Sanity check Answer relevance RAGAs: {sanity_check_answer_relevance_ragas:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "06f2c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(f\"results/sanity_check/results.json\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaffba2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
